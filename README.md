### Environment Setup

**Step 1**: Create a conda environment and activate it.

```bash
conda create -n fontdiffuser python=3.9 -y
conda activate fontdiffuser
```

**Step 2**: Install related version Pytorch following [here](https://pytorch.org/get-started/previous-versions/).

```bash
# Suggested
pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117
```

**Step 3**: Install the required packages.

```bash
pip install -r requirements.txt
```

**Step last-end**:

```bash
# å­—ä½“ç”Ÿæˆ
npm install
apt-get install python3-fontforge

# å…¨æµç¨‹ç”Ÿæˆ æ‰“å¼€æ–‡ä»¶æŸ¥çœ‹ç”¨æ³•
run_all.py
# åŠæµç¨‹
run_gen.py
## ğŸ‹ï¸ Training
### Data Construction
The training data files tree should be (The data examples are shown in directory `data_examples/train/`):
```

```
â”œâ”€â”€data_examples
â”‚   â””â”€â”€ train
â”‚       â”œâ”€â”€ ContentImage
â”‚       â”‚   â”œâ”€â”€ char0.png
â”‚       â”‚   â”œâ”€â”€ char1.png
â”‚       â”‚   â”œâ”€â”€ char2.png
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ TargetImage.png
â”‚           â”œâ”€â”€ style0
â”‚           â”‚     â”œâ”€â”€style0+char0.png
â”‚           â”‚     â”œâ”€â”€style0+char1.png
â”‚           â”‚     â””â”€â”€ ...
â”‚           â”œâ”€â”€ style1
â”‚           â”‚     â”œâ”€â”€style1+char0.png
â”‚           â”‚     â”œâ”€â”€style1+char1.png
â”‚           â”‚     â””â”€â”€ ...
â”‚           â”œâ”€â”€ style2
â”‚           â”‚     â”œâ”€â”€style2+char0.png
â”‚           â”‚     â”œâ”€â”€style2+char1.png
â”‚           â”‚     â””â”€â”€ ...
â”‚           â””â”€â”€ ...
```

### Training Configuration

Before running the training script (including the following three modes), you should set the training configuration,
such as distributed training, through:

```bash
accelerate config
```

### Training - Pretraining of SCR

```bash
Coming Soon ...
```

### Training - Phase 1

```bash
sh scripts/train_phase_1.sh
```

- `data_root`: The data root, as `./data_examples`
- `output_dir`: The training output logs and checkpoints saving directory.
- `resolution`: The resolution of the UNet in our diffusion model.
- `style_image_size`: The resolution of the style image, can be different with `resolution`.
- `content_image_size`: The resolution of the content image, should be the same as the `resolution`.
- `channel_attn`: Whether to use the channel attention in the MCA block.
- `train_batch_size`: The batch size in the training.
- `max_train_steps`: The maximum of the training steps.
- `learning_rate`: The learning rate when training.
- `ckpt_interval`: The checkpoint saving interval when training.
- `drop_prob`: The classifier-free guidance training probability.

### Training - Phase 2

After the phase 2 training, you should put the trained checkpoint files (`unet.pth`, `content_encoder.pth`,
and `style_encoder.pth`) to the directory `phase_1_ckpt`. During phase 2, these parameters will be resumed.

```bash
sh scripts/train_phase_2.sh
```

- `phase_2`: Tag to phase 2 training.
- `phase_1_ckpt_dir`: The model checkpoints saving directory after phase 1 training.
- `scr_ckpt_path`: The ckpt path of pre-trained SCR module. You can download it from above ğŸ”¥Model Zoo.
- `sc_coefficient`: The coefficient of style contrastive loss for supervision.
- `num_neg`: The number of negative samples, default to be `16`.

## ğŸ“º Sampling

### Step 1 => Prepare the checkpoint

Option (1) Download the checkpoint
following [GoogleDrive](https://drive.google.com/drive/folders/12hfuZ9MQvXqcteNuz7JQ2B_mUcTr-5jZ?usp=drive_link) / [BaiduYun:gexg](https://pan.baidu.com/s/19t1B7le8x8L2yFGaOvyyBQ),
then put the `ckpt` to the root directory, including the files `unet.pth`, `content_encoder.pth`,
and `style_encoder.pth`.  
Option (2) Put your re-training checkpoint folder `ckpt` to the root directory, including the
files `unet.pth`, `content_encoder.pth`, and `style_encoder.pth`.

### Step 2 => Run the script

**(1) Sampling image from content image and reference image.**

```bash
sh script/sample_content_image.sh
```

- `ckpt_dir`: The model checkpoints saving directory.
- `content_image_path`: The content/source image path.
- `style_image_path`: The style/reference image path.
- `save_image`: set `True` if saving as images.
- `save_image_dir`: The image saving directory, the saving files including an `out_single.png` and an `out_with_cs.png`.
- `device`: The sampling device, recommended GPU acceleration.
- `guidance_scale`: The classifier-free sampling guidance scale.
- `num_inference_steps`: The inference step by DPM-Solver++.

**(2) Sampling image from content character.**  
**Note** Maybe you need a ttf file that contains numerous Chinese characters, you can download it
from [BaiduYun:wrth](https://pan.baidu.com/s/1LhcXG4tPcso9BLaUzU6KtQ).

```bash
sh script/sample_content_character.sh
```

- `character_input`: If set `True`, use character string as content/source input.
- `content_character`: The content/source content character string.
- The other parameters are the same as the above option (1).

## ğŸ“± Run WebUI

### (1) Sampling by FontDiffuser

```bash
python font_easy_ui.py
python font_complex_ui.py
```

```text
ç»™å‡ºæ–°çš„ä»£ç ,ä½¿å¾—é£æ ¼ç‰¹å¾åªéœ€è¦æå–ä¸€æ¬¡,ç„¶ååº”ç”¨åˆ°æ‰€æœ‰çš„å†…å®¹å›¾åƒä¸Š,éœ€è¦é¢„å…ˆè®¡ç®—çš„é£æ ¼æ½œåœ¨è¡¨ç¤º,åŒæ—¶ä¹Ÿéœ€è¦ä¿®æ”¹ FontDiffuserDPMPipeline ç±»

1.å¸®æˆ‘è§£é‡Šä¸€ä¸‹æ•´ä½“å­—ä½“é£æ ¼è¿ç§»çš„æ¶æ„ 2.ç»™å‡ºæ¯ä¸ªç±»å’Œä¸€äº›å…³é”®å‡½æ•°çš„å·¦å³ 3.ç»™å‡ºä»£ç æ‰§è¡Œæµå‘

```

```text
æ•´ä½“æ¶æ„:
ç³»ç»Ÿé‡‡ç”¨äº†æ‰©æ•£æ¨¡å‹(Diffusion Model)çš„æ¶æ„,ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®ç»„ä»¶:
UNet: æ ¸å¿ƒç½‘ç»œ,ç”¨äºç”Ÿæˆå™ªå£°é¢„æµ‹ã€‚
å†…å®¹ç¼–ç å™¨(Content Encoder): æå–å†…å®¹ç‰¹å¾ã€‚
é£æ ¼ç¼–ç å™¨(Style Encoder): æå–é£æ ¼ç‰¹å¾ã€‚
DPM Solver: ç”¨äºä»å™ªå£°ä¸­é‡‡æ ·ç”Ÿæˆå›¾åƒã€‚

FontDiffuserModelDPM (src/model.py):
è¿™æ˜¯æ•´ä¸ªæ¨¡å‹çš„æ ¸å¿ƒç±»,æ•´åˆäº†UNetã€å†…å®¹ç¼–ç å™¨å’Œé£æ ¼ç¼–ç å™¨ã€‚å…¶forwardæ–¹æ³•å¤„ç†è¾“å…¥,æå–ç‰¹å¾,å¹¶é€šè¿‡UNetç”Ÿæˆå™ªå£°é¢„æµ‹ã€‚
StyleEncoder (src/modules/style_encoder.py):
è´Ÿè´£æå–é£æ ¼å›¾åƒçš„ç‰¹å¾ã€‚
FontDiffuserDPMPipeline (batch_gen.py):
è¿™æ˜¯æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹çš„pipeline,åŒ…æ‹¬åŠ è½½æ¨¡å‹ã€å¤„ç†è¾“å…¥ã€è¿è¡Œæ‰©æ•£è¿‡ç¨‹å’Œä¿å­˜ç»“æœã€‚
ä½¿ç”¨DPM_Solverè°ƒåº¦å™¨å®ç°å›¾åƒç”Ÿæˆçš„å®Œæ•´æµç¨‹ã€‚generateæ–¹æ³•å®ç°ä»é«˜æ–¯å™ªå£°åˆ°æœ€ç»ˆå›¾åƒçš„è½¬åŒ–
train.py è®­ç»ƒ:
è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹æ„å»ºã€ä¼˜åŒ–å™¨è®¾ç½®å’Œè®­ç»ƒå¾ªç¯ã€‚ä½¿ç”¨äº†Acceleratoræ¥æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒã€‚


FontDiffuserModel
- è´Ÿè´£ä½¿ç”¨UNetã€é£æ ¼ç¼–ç å™¨å’Œå†…å®¹ç¼–ç å™¨è¿›è¡Œæ­£å‘æ¨ç†ã€‚
- `forward`æ–¹æ³•æ‰§è¡Œå›¾åƒçš„å™ªå£°é¢„æµ‹å¹¶è¿”å›å™ªå£°é¢„æµ‹å’Œåç§»è¾“å‡ºæ€»å’Œã€‚
NoiseScheduleVP
- å®šä¹‰æ­£å‘SDEï¼ˆå¦‚ç¦»æ•£å™ªå£°è®¡åˆ’ï¼‰æ‰€éœ€çš„ç³»æ•°è®¡ç®—ã€‚
DPM_Solver
- å®ç°DPM-Solverå’ŒDPM-Solver++ç®—æ³•ä»¥è§£å†³SDEã€‚
- singlestep_dpm_solver_updateå’Œmultistep_dpm_solver_updateç­‰æ–¹æ³•ç”¨äºè§£ç®—å…·ä½“çš„æ›´æ–°æ­¥éª¤ã€‚
```
